{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics in Context | A Proof of Concept\n",
    "\n",
    "### Describing Citation Data with [Frictionless](https://frictionlessdata.io/)\n",
    "\n",
    "*Asura Enkhbayar, 04.02.2021*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents a proof of concept for a standardized approach to citation data and, more broadly, scholarly metrics. To do so, we will take a bibliometric dataset consisting of four spreadsheets and build a Frictionless [Data Package](https://specs.frictionlessdata.io/data-package/) using a systematic approach to describe each tables provenance. I will first introduce the dataset at hand and provide an quick overview and situate it in the context of traditional bibliometric work. Then, I will step-by-step introduce a new conceptual framework for scholarly metrics and gradually incorporate the dataset into the final data package. Finally, a few concluding remarks and a brief discussion of the potential and limitations of this approach.\n",
    "\n",
    "*Note: For a broader overview of the project please refer to the [README](https://github.com/Bubblbu/metrics-in-context) in the project repository.*\n",
    "\n",
    "<img style=\"float: center;\" width=25%, src=\"../../materials/assets/fosdem2021.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "Let's quickly import a few libraries and functions and setup our folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "from frictionless import describe_schema, describe_resource\n",
    "\n",
    "# Directories\n",
    "input_files = Path(\"./input_files\")\n",
    "schemas = Path(\"./schemas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this particular notebook, I will not go into the nitty gritty details of this dataset. Of course, providing that kind of information would be part of the aims of developing the Frictionless Data Package for scholarly metrics as these details make up provenance information. For now, I will simply describe the spreadsheets at hand and provide some  cursory descriptions of its origins.\n",
    "\n",
    "The dataset we are working with consists of four spreadsheets that were created based on data provided by [Scite.ai](https://scite.ai/). The particular sample was created by querying for recent articles in Pubmed on Amyotrophic lateral sclerosis (ALS). Scite is one of the newer data sources that not only provide the classic citation link between two documents, but attempt to extract what has been established as citation contexts. While the former can be understood as the datafication of bibliographies and reference lists, context-aware citations trace each individual in-text mention of articles.\n",
    "\n",
    "The four CSVs are:\n",
    "\n",
    "- `contexts.csv`: Article metadata for each DOI.\n",
    "- `traces.csv`: The individual traced citations between documents and context information such as a text snippet or the mentioning section.\n",
    "- `citations.csv`: This spreadsheet contains metrics for every individual citation link derived from its source article such as the total number of outgoing references.\n",
    "- `articles.csv`: A table with article-level metrics derived from the aforementioned traces.\n",
    "\n",
    "Very often, bibliometric researchers and practictioners will only get to see the final article-level metrics, i.e., the citation counts for DOIs. In the next sections, we will now gradually go through these spreadsheets and describe how citation counts become what they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling A Scholarly Metrics Data Package\n",
    "\n",
    "Here, I want to attempt to briefly introduce the conceptual framework used to systematize scholarly metrics. This framework is part of a broader doctoral research project and is built around the philosophical shift from object-centrism to process-centrism. This shift from representation towards performance can also be expressed for scholarly metrics by emphasising the processes and practices that lead to outputs and citation counts. But how do these processes look like and how can we start to systematically describe them?\n",
    "\n",
    "The usual story goes something like this: A *citation* links two scholarly texts expressed through an in-text mention and a bibliographic reference. This state is then captured by a *citation link* in their databases. This data then enables the creation of *citation metrics*. I would like to retell this story equipped with a new vocabulary for provenance and while paying attention to each step in the individuation of the citation count.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Citational Event\n",
    "\n",
    "> A *citation* links two *scholarly texts* expressed through an in-text mention and a bibliographic reference.\n",
    "\n",
    "Firstly, I propose to question the idea of the citation as one consistent concept itself. It is typically understood as a directional binary property between two peer-reviewed research articles. However, more and more citation databases are using text-processing methods to extract the individual in-text mentions with their contexts and furthermore the types of documents hosting citations is changing (e.g., datasets, software, mentions in social media). To accomodate for these changes, I suggest to think in terms of *citational events* and their *contexts*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical citation is a statement in a peer-reviewed scholarly article referencing another piece of scholarly writing. Some of the new social (and technical) challenges around citations stem from changing citational practices such as the citing of preprints or datasets. So far, these changes have been pragmatically addressed by introduction of norms and dedicated infrastructure (e.g., the push for DOIs for datasets and iniatives that track their usage) but we are lacking the conceptual language to capture all these forms of citations.\n",
    "\n",
    "To avoid falling into the trap of never-ending definitions, I propose to re-conceptualize the citation as an interdiscursive event (Nakassis, 2013) which links two discursive acts. For the present purpose it will suffice to understand *discursive acts* as some form of written or spoken statement. Modality, syntax, and even semantics become part of the wider context of that particular citational event. I've talked enough about things that bald French men usually talk about, thus, without further ado, let's dive into implementing said citational event:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "type: citational\n",
    "description: A textual reference from one scholarly document to another scholarly document.\n",
    "source_event:\n",
    "  activity: scholarly writing\n",
    "target_event:\n",
    "  activity: scholarly writing\n",
    "```\n",
    "\n",
    "Scite captures *citational* events. Other services also capture events which are not citational such as views, downloads, or bookmarks. Throughout this document, technical objects will always be accompanied with free-text descriptions. Finally, we also attempt to describe the source and target events individually. In our case, both the citing as well as the cited events are caused by scholarly writing. Altmetrics would consider source events caused by social media activities referencing scholarly texts.\n",
    "\n",
    "*Note: This might seem like a very lean definition of the fundamental entity that is being captured. This is on purpose as we will continue to explore the complexity through the additional processes of becoming the final metric.*\n",
    "\n",
    "*Another note: This schema implemented in YAML is only modelling the captured event. It does not have a representation in data as they represent the real world thing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contexts**\n",
    "\n",
    "These citational events do not occur in a vacuum. As already indicated earlier, they typically happen in contexts which are those things that we usually pay attention to most. The research articles, conference papers, literature reviews, white papers, preprints, and other formats that nowadays host citations of all kinds are the typical unit of analysis when it comes to scholarly metrics. However, with this small committment to the event as the core of this framework, I am hoping to shift attention to the processes and practices leading to citations rather than their manifested forms. Still, we obviously still need to talk and specify these concrete outputs that we love and hate so much. The following yaml excerpt is an example for a list of possible contexts for Scite:\n",
    "\n",
    "```yaml\n",
    "contexts:\n",
    "    - type: peer-reviewed articles\n",
    "      coverage:\n",
    "        - publishers sharing fulltexts with Scite\n",
    "        - Pubmed Open Access Subset\n",
    "        - OA articles accessed through Unpaywall\n",
    "      identified_by:\n",
    "        - DOI\n",
    "    - type: preprints\n",
    "      coverage:\n",
    "        - bioRxiv\n",
    "        - medRxiv\n",
    "      identified_by:\n",
    "        - DOI\n",
    "```\n",
    "\n",
    "In contrast to previously modelled events, we do have a data on the contexts modelled by this schema: `input_files/contexts.csv` which is basically a spreadsheet with article metadata. Let's see how we can combine these two using Frictionless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "scite_contexts = describe_resource(\"input_files/contexts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the helper function `describe_resource` to get a headstart, we can continue to set some project specific properties. Furthermore, we can also expand and improve the table schema of this resource which describes the actual data columns in the resource. For instance, the CSV contains one article per row identified by the \"doi\" column. Accordingly, we can set \"doi\" as the primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some general properties\n",
    "scite_contexts.name = \"scite_contexts\"\n",
    "scite_contexts.profile = \"mic-contexts\" # this profile does not exist yet\n",
    "scite_contexts.description = \"Context metadata from Scite.ai | articles identified by DOI\"\n",
    "\n",
    "# Set the primary key\n",
    "scite_contexts.schema.primary_key = \"doi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will add a new property `prov` which we will populate with the previously created schema for Scite contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "scite_contexts['prov'] = Schema(\"schemas/contexts.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, our final result of this data resource looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'compression': 'no',\n",
      " 'compressionPath': '',\n",
      " 'control': {'newline': ''},\n",
      " 'description': 'Context metadata from Scite.ai | articles identified by DOI',\n",
      " 'dialect': {'quoteChar': '\"'},\n",
      " 'encoding': 'utf-8',\n",
      " 'format': 'csv',\n",
      " 'hashing': 'md5',\n",
      " 'name': 'scite_contexts',\n",
      " 'path': 'input_files/contexts.csv',\n",
      " 'profile': 'mic-contexts',\n",
      " 'prov': {'contexts': [{'coverage': ['publishers sharing fulltexts with Scite',\n",
      "                                     'Pubmed Open Access Subset',\n",
      "                                     'OA articles accessed through Unpaywall'],\n",
      "                        'identified_by': ['DOI'],\n",
      "                        'type': 'peer-reviewed articles'},\n",
      "                       {'coverage': ['bioRxiv', 'medRxiv'],\n",
      "                        'identified_by': ['DOI'],\n",
      "                        'type': 'preprints'}]},\n",
      " 'query': {},\n",
      " 'schema': {'fields': [{'name': 'doi', 'type': 'string'},\n",
      "                       {'name': 'slug', 'type': 'string'},\n",
      "                       {'name': 'type', 'type': 'string'},\n",
      "                       {'name': 'title', 'type': 'string'},\n",
      "                       {'name': 'abstract', 'type': 'string'},\n",
      "                       {'name': 'authors', 'type': 'string'},\n",
      "                       {'name': 'keywords', 'type': 'string'},\n",
      "                       {'name': 'year', 'type': 'number'},\n",
      "                       {'name': 'shortJournal', 'type': 'string'},\n",
      "                       {'name': 'publisher', 'type': 'string'},\n",
      "                       {'name': 'issue', 'type': 'string'},\n",
      "                       {'name': 'volume', 'type': 'integer'},\n",
      "                       {'name': 'page', 'type': 'string'},\n",
      "                       {'name': 'retracted', 'type': 'boolean'},\n",
      "                       {'name': 'memberId', 'type': 'integer'},\n",
      "                       {'name': 'issns', 'type': 'string'},\n",
      "                       {'name': 'editorialNotices', 'type': 'string'},\n",
      "                       {'name': 'journalSlug', 'type': 'string'},\n",
      "                       {'name': 'journal', 'type': 'string'},\n",
      "                       {'name': 'rwStatus', 'type': 'any'}],\n",
      "            'primaryKey': 'doi'},\n",
      " 'scheme': 'file',\n",
      " 'stats': {'bytes': 22627600,\n",
      "           'fields': 20,\n",
      "           'hash': '13a45978aa0e8c616ebd5a0c6a0ccb1c',\n",
      "           'rows': 10850}}\n"
     ]
    }
   ],
   "source": [
    "pprint(scite_contexts, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might not look like much so far, but we have successfully loaded a spreadsheet containing article metadata into a data structure which provides mechanisms to describe:\n",
    "\n",
    "1. The internal structure and logics of the spreadsheet using `schema`\n",
    "2. The kind of contexts that are captured using the new `prov` property, i.e., **we are doing provenance!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Tracing of Events and Contexts\n",
    "\n",
    "> This state is then captured by a *citation link* in their databases.\n",
    "\n",
    "This might be the most overlooked and undertheorized aspect of modern citations. Scholars have been discussing the meaning and nature of the citation for decades, bibliometricians have developed, critiqued, and improved various scholarly metrics based on citation data, and similary, the application and usage of those metrics is also widely discussed and scrutinized. But what about the tracing of citational events and their way into the databases of knowledge?\n",
    "\n",
    "To answer this question, I want to introduce *tracing* processes which are ways of creating an imprint or a *trace* of a citational event. Citation indexes are the obvious example for institutions that trace citational events by systematically documenting the cited and citing documents. In the recent years, however, the number of methods of tracing is growing with the number citation data providers most of which do not share their source code.\n",
    "\n",
    "One of the reasons why processes of tracing haven't been scrutitinized in detail might be related to the fact that most of them are black boxes. However, in an attempt to start these conversations, in the next steps of this project, we will begin to outline the individual black-boxes and put labels on them. This exercises value becomes especially clear once we start to apply the same vocabulary to the newer open initiatives such as [OpenCitations](http://opencitations.net/index/coci). For now, I will provide an exemplary tracing pipeline (input from the Scite.ai folks would be highly appreciated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "event: schemas/event.yaml\n",
    "contexts: schemas/contexts.yaml\n",
    "tracing:\n",
    "  pipeline:\n",
    "    - citation_extraction: text-processing/ML\n",
    "    - citation_reference_matching: text-processing/ML\n",
    "```\n",
    "\n",
    "By including the detailed data collection and analytic processes involved we are now starting to think processually. Citational traces provided by indexing services are fundamentally different, not only because of diverging article coverages, but also because these tracing processes are made up of different items.\n",
    "\n",
    "Similarly to contexts, we do have a corresponding CSV again for these traces: `input_files/traces.csv` which simply reports every single occurance of citing-cited-article pairs. Let's load it into a Frictionless resource using the same helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "scite_traces = describe_resource(\"input_files/traces.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time our resource object is even simpler as the spreadsheet contains five columns: `id`, `source`, `target`, `snippet`, and `section`. We will once again set some basic properties like a name, descriptor, and a profile, and a primary key on `id`. However, in addition we can designate both `source` and `target` as foreign keys as the each DOI is also in the contexts resource which contains all metadata. Both `snippet` and `target` refer to the actual textual contexts that the mentions happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some general properties\n",
    "scite_traces.name = \"scite_traces\"\n",
    "scite_traces.profile = \"mic-traces\"\n",
    "scite_traces.description = \"Traced citing-cited article pairs for each mention\"\n",
    "\n",
    "# Extend the table schema\n",
    "scite_traces.schema.primary_key = \"doi\"\n",
    "scite_traces.schema.foreign_keys.append(\n",
    "    {\"fields\": [\"source\"], \"reference\": {\"resource\": \"scite_contexts\", \"fields\": [\"id\"]}}\n",
    ")\n",
    "scite_traces.schema.foreign_keys.append(\n",
    "    {\"fields\": [\"target\"], \"reference\": {\"resource\": \"scite_contexts\", \"fields\": [\"id\"]}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing missing is the provenance information. Just as we did last time, we are going to use the `prov` property to add our experimental trace schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "scite_traces['prov'] = Schema(\"schemas/traces.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'compression': 'no',\n",
      " 'compressionPath': '',\n",
      " 'control': {'newline': ''},\n",
      " 'description': 'Traced citing-cited article pairs for each mention',\n",
      " 'dialect': {},\n",
      " 'encoding': 'utf-8',\n",
      " 'format': 'csv',\n",
      " 'hashing': 'md5',\n",
      " 'name': 'scite_traces',\n",
      " 'path': 'input_files/traces.csv',\n",
      " 'profile': 'mic-traces',\n",
      " 'prov': {'contexts': 'schemas/contexts.yaml',\n",
      "          'event': 'schemas/event.yaml',\n",
      "          'tracing': {'pipeline': [{'citation_extraction': 'text-processing/ML'},\n",
      "                                   {'citation_reference_matching': 'text-processing/ML'}]}},\n",
      " 'query': {},\n",
      " 'schema': {'fields': [{'name': 'id', 'type': 'integer'},\n",
      "                       {'name': 'source', 'type': 'string'},\n",
      "                       {'name': 'target', 'type': 'string'},\n",
      "                       {'name': 'snippet', 'type': 'any'},\n",
      "                       {'name': 'section', 'type': 'any'}],\n",
      "            'foreignKeys': [{'fields': ['source'],\n",
      "                             'reference': {'fields': ['id'],\n",
      "                                           'resource': 'scite_contexts'}},\n",
      "                            {'fields': ['target'],\n",
      "                             'reference': {'fields': ['id'],\n",
      "                                           'resource': 'scite_contexts'}}],\n",
      "            'primaryKey': 'doi'},\n",
      " 'scheme': 'file',\n",
      " 'stats': {'bytes': 64458354,\n",
      "           'fields': 5,\n",
      "           'hash': 'c1b7df839157e7729e5678defd68e593',\n",
      "           'rows': 1048575}}\n"
     ]
    }
   ],
   "source": [
    "pprint(scite_traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time our data resource already contains considerably more complexity:\n",
    "    \n",
    "1. The internal structure of the CSV and its columns is no longer limited to this one resource. We have added foreignKeys which reference rows in the `scite_contexts` resource.\n",
    "2. The provenance information is also relational. Not only have we defined that the kind of captured events (`schemas/events.yaml` which we wrote earlier) but also the contexts in which we are actually looking for these events (`schemas/contexts.yaml`). Finally, we have started to describe parts of the tracing pipeline in very rudimentary terms.\n",
    "\n",
    "**Look mum, more provenance!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Patterns and Metrics [CURRENT PROGRESS]\n",
    "\n",
    "> This data then enables the creation of *citation metrics*.\n",
    "\n",
    "So far we have talked about events, contexts, and traces but haven't encountered any metrics such as the citation count.\n",
    "\n",
    "**Patterns**\n",
    "\n",
    "\n",
    "\n",
    "All metrics are patterns, but not all patterns are metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling the Final Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Package(resources=[scite_contexts, scite_traces, citation_patters, article_patterns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article level patterns\n",
    "\n",
    "This table contains citation patterns aggregated on the article level.\n",
    "\n",
    "Each table will thus contain:\n",
    "\n",
    "- A unique DOI\n",
    "- One or more patterns for each DOI\n",
    "    - Each pattern also contain the form of aggregation and input traces/patterns\n",
    "        - Optional for each input trace/pattern: a data resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"article_patterns.csv\"\n",
    "article_patterns = pd.read_csv(f)\n",
    "ap_schema = describe_schema(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-41b5306200e4>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-41b5306200e4>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    \"type\": \"aggregation\",\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Set primary key\n",
    "ap_schema.primary_key = \"doi\"\n",
    "ap_schema.get_field(\"doi\").title = \"DOI of citing article\"\n",
    "\n",
    "# First pattern: mentions_agg\n",
    "mentions_agg = ap_schema.get_field(\"mentions_agg\")\n",
    "mentions_agg.title = \"Aggregated mentions\"\n",
    "mentions_agg.description = \"Sum of all incoming mentions for this DOI\"\n",
    "mentions_agg.type = \"integer\"\n",
    "mentions_agg.missing_values = [\"\", \"n/a\", \"NaN\"]\n",
    "mentions_agg.mic = {\n",
    "    \"type\": \"mic-pattern\",\n",
    "    \"prov\": {\n",
    "        \"operation\": {\n",
    "            \"description\": \"Aggregation of all citations by DOI\"\n",
    "            \"type\": \"aggregation\",\n",
    "            \"by\": \"doi\",\n",
    "            \"resource\": \"\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Second pattern: refs_agg\n",
    "refs_agg = ap_schema.get_field(\"refs_agg\")\n",
    "refs_agg.title = \"Aggregated references\"\n",
    "refs_agg.description = \"Sum of all incoming references for this DOI\"\n",
    "refs_agg.type = \"integer\"\n",
    "refs_agg.missing_values = [\"\", \"n/a\", \"NaN\"]\n",
    "refs_agg.mic = {\n",
    "    \"type\": \"pattern\",\n",
    "    \"prov\": {\n",
    "        \"operation\": {\n",
    "            \"type\": \"aggregation\",\n",
    "            \"by\": \"doi\",\n",
    "            \"input\": \"\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add third pattern which is an average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource specs\n",
    "resource_desc = {\n",
    "    \"profile\": \"pattern\",\n",
    "    \"name\": \"Article-level metrics\",\n",
    "    \"data\": data_dir / \"article_patterns.csv\",\n",
    "    \"schema\": ap_schema\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- Also provides a logical structure for bibliometrics datasets\n",
    "- By splitting data resources and schemas it is possible to discuss provenance information without having actual data of any prior stages and processes\n",
    "- Frictionless enables the addition of logic and function to the data packages which opens door for extensions:\n",
    "    - Automatically retrieve metadata and citations from open services (Crossref/COCI) including their provenance schemas of course\n",
    "    - Compare metrics in a dataset and automatically point out incommensurable fields\n",
    "    - Automatically emphasize black-boxes in the data processing pipelines\n",
    "    - Create visualizations of these provenance pipelines\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Nakassis, C. V. (2013). Citation and Citationality. Signs and Society, 1(1), 51–77. https://doi.org/10.1086/670165"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrics-in-context",
   "language": "python",
   "name": "metrics-in-context"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
